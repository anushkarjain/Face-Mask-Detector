{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict={\"with_mask\":0, \"without_mask\":1}  #dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=[\"with_mask\",\"without_mask\"]       #list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"C:\\\\Users\\\\anush\\\\Documents\\\\dataset\"         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2,os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]    \n",
    "target=[]     #empty lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "  folder_path=os.path.join(data_path,category)\n",
    "  img_names=os.listdir(folder_path)\n",
    "  for img_name in img_names:\n",
    "    img_path=os.path.join(folder_path,img_name)\n",
    "    img=cv2.imread(img_path)\n",
    "    try:\n",
    "      gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "      resized=cv2.resize(gray,(100,100))\n",
    "      data.append(resized)\n",
    "      target.append(label_dict[category])\n",
    "    except Exception as e:\n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "drOVS6yI8N2y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data=np.array(data)\n",
    "data=data/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.2       , 0.20392157, 0.25098039, ..., 0.33333333,\n",
       "         0.03529412, 0.12941176],\n",
       "        [0.2       , 0.21568627, 0.24313725, ..., 0.07843137,\n",
       "         0.23529412, 0.15686275],\n",
       "        [0.21176471, 0.20392157, 0.18823529, ..., 0.27843137,\n",
       "         0.20392157, 0.01568627],\n",
       "        ...,\n",
       "        [0.29019608, 0.30196078, 0.2745098 , ..., 0.05490196,\n",
       "         0.01960784, 0.2       ],\n",
       "        [0.25098039, 0.26666667, 0.29019608, ..., 0.01960784,\n",
       "         0.04705882, 0.03529412],\n",
       "        [0.23529412, 0.23529412, 0.21960784, ..., 0.05882353,\n",
       "         0.05882353, 0.05882353]],\n",
       "\n",
       "       [[0.98039216, 0.98039216, 0.98431373, ..., 0.98431373,\n",
       "         0.98431373, 0.98431373],\n",
       "        [0.98039216, 0.98039216, 0.98431373, ..., 0.98431373,\n",
       "         0.98431373, 0.98431373],\n",
       "        [0.98431373, 0.98431373, 0.98431373, ..., 0.98823529,\n",
       "         0.98823529, 0.98823529],\n",
       "        ...,\n",
       "        [1.        , 1.        , 1.        , ..., 0.09019608,\n",
       "         0.08235294, 0.08627451],\n",
       "        [1.        , 1.        , 1.        , ..., 0.08627451,\n",
       "         0.08235294, 0.08627451],\n",
       "        [1.        , 1.        , 1.        , ..., 0.08235294,\n",
       "         0.08235294, 0.08627451]],\n",
       "\n",
       "       [[0.8745098 , 0.88235294, 0.88627451, ..., 0.78431373,\n",
       "         0.76470588, 0.74509804],\n",
       "        [0.88627451, 0.89019608, 0.89803922, ..., 0.8       ,\n",
       "         0.77647059, 0.76470588],\n",
       "        [0.89411765, 0.89803922, 0.90196078, ..., 0.80784314,\n",
       "         0.79215686, 0.77647059],\n",
       "        ...,\n",
       "        [0.78431373, 0.80784314, 0.78039216, ..., 0.15686275,\n",
       "         0.15686275, 0.11372549],\n",
       "        [0.68235294, 0.7372549 , 0.77254902, ..., 0.16862745,\n",
       "         0.12941176, 0.11764706],\n",
       "        [0.63529412, 0.69803922, 0.72941176, ..., 0.14509804,\n",
       "         0.10980392, 0.10196078]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.90980392, 0.92941176, 0.98039216, ..., 0.87058824,\n",
       "         0.85882353, 0.8       ],\n",
       "        [0.84313725, 0.91764706, 0.96470588, ..., 0.97254902,\n",
       "         0.94901961, 0.87843137],\n",
       "        [0.89411765, 0.85882353, 0.98039216, ..., 0.9254902 ,\n",
       "         0.82352941, 0.85098039],\n",
       "        ...,\n",
       "        [0.1372549 , 0.10980392, 0.17647059, ..., 0.08235294,\n",
       "         0.89411765, 0.94117647],\n",
       "        [0.03921569, 0.0745098 , 0.02352941, ..., 0.07058824,\n",
       "         0.42352941, 0.92156863],\n",
       "        [0.09411765, 0.08627451, 0.05882353, ..., 0.07058824,\n",
       "         0.07058824, 0.89411765]],\n",
       "\n",
       "       [[0.9254902 , 0.96078431, 0.95294118, ..., 0.90588235,\n",
       "         0.90196078, 0.79215686],\n",
       "        [0.94117647, 0.89019608, 0.92941176, ..., 0.88627451,\n",
       "         0.88627451, 0.89019608],\n",
       "        [0.95294118, 0.87843137, 0.94901961, ..., 0.94509804,\n",
       "         0.90196078, 0.89803922],\n",
       "        ...,\n",
       "        [0.0745098 , 0.17254902, 0.15686275, ..., 0.02745098,\n",
       "         0.90196078, 0.95294118],\n",
       "        [0.12941176, 0.05882353, 0.03529412, ..., 0.08627451,\n",
       "         0.44705882, 0.98039216],\n",
       "        [0.07843137, 0.09411765, 0.08235294, ..., 0.1372549 ,\n",
       "         0.09803922, 0.91372549]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1376, 100, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.reshape(data,(data.shape[0],100,100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1376, 100, 100, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1376,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_target=np_utils.to_categorical(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1376, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import Conv2D,MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(200,(3,3),input_shape=data.shape[1:], activation = \"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(100,(3,3), activation = \"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data,train_target,test_target =train_test_split(data,new_target,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.7402 - accuracy: 0.5475WARNING:tensorflow:From C:\\Users\\anush\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\anush\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: model-001.model\\assets\n",
      "31/31 [==============================] - 81s 3s/step - loss: 0.7402 - accuracy: 0.5475 - val_loss: 0.7040 - val_accuracy: 0.4879\n",
      "Epoch 2/30\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.5647 - accuracy: 0.7040INFO:tensorflow:Assets written to: model-002.model\\assets\n",
      "31/31 [==============================] - 92s 3s/step - loss: 0.5647 - accuracy: 0.7040 - val_loss: 0.5475 - val_accuracy: 0.7339\n",
      "Epoch 3/30\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4113 - accuracy: 0.8020INFO:tensorflow:Assets written to: model-003.model\\assets\n",
      "31/31 [==============================] - 74s 2s/step - loss: 0.4113 - accuracy: 0.8020 - val_loss: 0.4020 - val_accuracy: 0.8387\n",
      "Epoch 4/30\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.2730 - accuracy: 0.8990INFO:tensorflow:Assets written to: model-004.model\\assets\n",
      "31/31 [==============================] - 75s 2s/step - loss: 0.2730 - accuracy: 0.8990 - val_loss: 0.3022 - val_accuracy: 0.8790\n",
      "Epoch 5/30\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.1901 - accuracy: 0.9313INFO:tensorflow:Assets written to: model-005.model\\assets\n",
      "31/31 [==============================] - 77s 2s/step - loss: 0.1901 - accuracy: 0.9313 - val_loss: 0.2445 - val_accuracy: 0.8952\n",
      "Epoch 6/30\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.1668 - accuracy: 0.9313INFO:tensorflow:Assets written to: model-006.model\\assets\n",
      "31/31 [==============================] - 76s 2s/step - loss: 0.1668 - accuracy: 0.9313 - val_loss: 0.1959 - val_accuracy: 0.9194\n",
      "Epoch 7/30\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.1292 - accuracy: 0.9515INFO:tensorflow:Assets written to: model-007.model\\assets\n",
      "31/31 [==============================] - 76s 2s/step - loss: 0.1292 - accuracy: 0.9515 - val_loss: 0.1787 - val_accuracy: 0.9435\n",
      "Epoch 8/30\n",
      "31/31 [==============================] - 74s 2s/step - loss: 0.0918 - accuracy: 0.9657 - val_loss: 0.1874 - val_accuracy: 0.9073\n",
      "Epoch 9/30\n",
      "31/31 [==============================] - 72s 2s/step - loss: 0.1017 - accuracy: 0.9616 - val_loss: 0.3933 - val_accuracy: 0.8669\n",
      "Epoch 10/30\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.0885 - accuracy: 0.9747INFO:tensorflow:Assets written to: model-010.model\\assets\n",
      "31/31 [==============================] - 75s 2s/step - loss: 0.0885 - accuracy: 0.9747 - val_loss: 0.1662 - val_accuracy: 0.9435\n",
      "Epoch 11/30\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.9828INFO:tensorflow:Assets written to: model-011.model\\assets\n",
      "31/31 [==============================] - 77s 2s/step - loss: 0.0539 - accuracy: 0.9828 - val_loss: 0.1286 - val_accuracy: 0.9556\n",
      "Epoch 12/30\n",
      "31/31 [==============================] - 72s 2s/step - loss: 0.0449 - accuracy: 0.9818 - val_loss: 0.2067 - val_accuracy: 0.9315\n",
      "Epoch 13/30\n",
      "31/31 [==============================] - 71s 2s/step - loss: 0.0467 - accuracy: 0.9828 - val_loss: 0.3228 - val_accuracy: 0.9113\n",
      "Epoch 14/30\n",
      "31/31 [==============================] - 73s 2s/step - loss: 0.0970 - accuracy: 0.9596 - val_loss: 0.1665 - val_accuracy: 0.9435\n",
      "Epoch 15/30\n",
      "31/31 [==============================] - 72s 2s/step - loss: 0.0577 - accuracy: 0.9808 - val_loss: 0.1477 - val_accuracy: 0.9395\n",
      "Epoch 16/30\n",
      "31/31 [==============================] - 73s 2s/step - loss: 0.0574 - accuracy: 0.9818 - val_loss: 0.1441 - val_accuracy: 0.9516\n",
      "Epoch 17/30\n",
      "31/31 [==============================] - 73s 2s/step - loss: 0.0530 - accuracy: 0.9778 - val_loss: 0.1877 - val_accuracy: 0.9315\n",
      "Epoch 18/30\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9899INFO:tensorflow:Assets written to: model-018.model\\assets\n",
      "31/31 [==============================] - 76s 2s/step - loss: 0.0292 - accuracy: 0.9899 - val_loss: 0.1220 - val_accuracy: 0.9677\n",
      "Epoch 19/30\n",
      "31/31 [==============================] - 75s 2s/step - loss: 0.0213 - accuracy: 0.9949 - val_loss: 0.1950 - val_accuracy: 0.9395\n",
      "Epoch 20/30\n",
      "31/31 [==============================] - 76s 2s/step - loss: 0.0222 - accuracy: 0.9939 - val_loss: 0.1358 - val_accuracy: 0.9597\n",
      "Epoch 21/30\n",
      "31/31 [==============================] - 71s 2s/step - loss: 0.0176 - accuracy: 0.9939 - val_loss: 0.1586 - val_accuracy: 0.9556\n",
      "Epoch 22/30\n",
      "31/31 [==============================] - 73s 2s/step - loss: 0.0210 - accuracy: 0.9919 - val_loss: 0.1463 - val_accuracy: 0.9637\n",
      "Epoch 23/30\n",
      "31/31 [==============================] - 72s 2s/step - loss: 0.0293 - accuracy: 0.9879 - val_loss: 0.1368 - val_accuracy: 0.9637\n",
      "Epoch 24/30\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9929INFO:tensorflow:Assets written to: model-024.model\\assets\n",
      "31/31 [==============================] - 81s 3s/step - loss: 0.0237 - accuracy: 0.9929 - val_loss: 0.1038 - val_accuracy: 0.9677\n",
      "Epoch 25/30\n",
      "31/31 [==============================] - 69s 2s/step - loss: 0.0139 - accuracy: 0.9949 - val_loss: 0.1269 - val_accuracy: 0.9677\n",
      "Epoch 26/30\n",
      "31/31 [==============================] - 74s 2s/step - loss: 0.0136 - accuracy: 0.9960 - val_loss: 0.1397 - val_accuracy: 0.9476\n",
      "Epoch 27/30\n",
      "31/31 [==============================] - 64s 2s/step - loss: 0.0195 - accuracy: 0.9929 - val_loss: 0.1300 - val_accuracy: 0.9677\n",
      "Epoch 28/30\n",
      "31/31 [==============================] - 65s 2s/step - loss: 0.0151 - accuracy: 0.9939 - val_loss: 0.1276 - val_accuracy: 0.9718\n",
      "Epoch 29/30\n",
      "31/31 [==============================] - 61s 2s/step - loss: 0.0164 - accuracy: 0.9960 - val_loss: 0.1372 - val_accuracy: 0.9677\n",
      "Epoch 30/30\n",
      "31/31 [==============================] - 64s 2s/step - loss: 0.0119 - accuracy: 0.9980 - val_loss: 0.1403 - val_accuracy: 0.9516\n"
     ]
    }
   ],
   "source": [
    "checkpoint=ModelCheckpoint(\"model-{epoch:03d}.model\", save_best_only=True,mode=\"auto\")\n",
    "history=model.fit(train_data,train_target,epochs=30,validation_split=0.2,callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascader=cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=cv2.imread(\"C:\\\\Users\\\\anush\\\\Desktop\\\\Anushka.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "faces=face_cascader.detectMultiScale(img,1.3,5)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[369, 526, 172, 172]], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3654,
     "status": "error",
     "timestamp": 1599924863199,
     "user": {
      "displayName": "Anushka Jain",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgMhIr5Rdl1imgRsxOF7C5MbcQJgpSoZOC-NBBb_aE=s64",
      "userId": "17508243848514300602"
     },
     "user_tz": -330
    },
    "id": "mBUVNNrEAuBU",
    "outputId": "e5d5a9ef-f5cf-4912-a7a4-00cdfbd23145"
   },
   "outputs": [],
   "source": [
    "labels_dict={0:'MASK',1:'NO MASK'}\n",
    "color_dict={0:(0,255,0),1:(0,0,255)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "source=cv2.VideoCapture(0)\n",
    "while(True):\n",
    "\n",
    "    ret,img=source.read()\n",
    "    #gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces=face_cascader.detectMultiScale(img,1.3,5)  \n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "    \n",
    "        face_img=img[y:y+w,x:x+w]\n",
    "        resized=cv2.resize(face_img,(100,100))\n",
    "        #normalized=resized/255.0\n",
    "        \n",
    "        #result=model.predict(normalized)\n",
    "        normimage=resized/255\n",
    "        reshapeimage=np.reshape(normimage,(-1,100,100,1))\n",
    "        modelop=model.predict(reshapeimage)\n",
    "        \n",
    "        label=np.argmax(modelop,axis=1)[1]\n",
    "      \n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(img,(x,y-40),(x+w,y),color_dict[label],1)\n",
    "        \n",
    "        cv2.putText(img, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "       # cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "       # cv2.rectangle(img,(x,y-40),(x+w,y),(0,0,255),1)\n",
    "        \n",
    "        #cv2.putText(img, \"face\", (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "        \n",
    "    cv2.imshow(\"checking...\",img)\n",
    "    key=cv2.waitKey(2)\n",
    "    \n",
    "    if(key==27):\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "source.release()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMA+53gqDX0MobjRjOIFEBs",
   "mount_file_id": "1cy7yQvnhiqFvNZqpxiF-XRO3ObHDbKqG",
   "name": "AI-Major (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
